version: '3'

services:
  namenode:
    image: fixcer/namenode
    container_name: namenode
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env
    ports:
      - '9870:9870'
    networks:
      - net

  resourcemanager:
    image: fixcer/resourcemanager
    container_name: resourcemanager
    restart: on-failure
    depends_on:
      - namenode
      - datanode1
      - datanode2
      - datanode3
    env_file:
      - ./hadoop.env
    ports:
      - '8089:8088'
    networks:
      - net

  historyserver:
    image: fixcer/historyserver
    container_name: historyserver
    depends_on:
      - namenode
      - datanode1
      - datanode2
    volumes:
      - hadoop_historyserver:/hadoop/yarn/timeline
    env_file:
      - ./hadoop.env
    ports:
      - '8188:8188'
    networks:
      - net

  nodemanager:
    image: fixcer/nodemanager
    container_name: nodemanager
    depends_on:
      - namenode
      - datanode1
      - datanode2
    env_file:
      - ./hadoop.env
    ports:
      - '8042:8042'
    networks:
      - net

  datanode1:
    image: fixcer/datanode
    container_name: datanode1
    depends_on:
      - namenode
    volumes:
      - hadoop_datanode1:/hadoop/dfs/data
    env_file:
      - ./hadoop.env
    networks:
      - net

  datanode2:
    image: fixcer/datanode
    container_name: datanode2
    depends_on:
      - namenode
    volumes:
      - hadoop_datanode2:/hadoop/dfs/data
    env_file:
      - ./hadoop.env
    networks:
      - net

  datanode3:
    image: fixcer/datanode
    container_name: datanode3
    depends_on:
      - namenode
    volumes:
      - hadoop_datanode3:/hadoop/dfs/data
    env_file:
      - ./hadoop.env
    networks:
      - net

  spark-master:
    image: fixcer/spark-master
    container_name: spark_master
    ports:
      - 8080:8080
      - 7077:7077
    env_file:
      - ./hadoop.env
    networks:
      - net

  spark-worker:
    image: fixcer/spark-worker
    container_name: spark_worker
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    ports:
      - 8081:8081
    env_file:
      - ./hadoop.env
    networks:
      - net

  zookeeper:
    image: wurstmeister/zookeeper
    container_name: zookeeper
    expose:
      - '2181'
    ports:
      - 2181:2181
    networks:
      - net

  kafka:
    image: wurstmeister/kafka
    container_name: kafka
    expose:
      - '9092'
    ports:
      - 9092:9092
    links:
      - zookeeper
    depends_on:
      - zookeeper
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_PORT: 9092
      KAFKA_ADVERTISED_HOST_NAME: kafka
      KAFKA_ADVERTISED_LISTENERS: INSIDE://:9092,OUTSIDE://localhost:19092
      KAFKA_LISTENERS: INSIDE://:9092,OUTSIDE://0.0.0.0:19092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
    networks:
      - net

  kafka_manager:
    image: hlebalbau/kafka-manager:1.3.3.18
    container_name: kafka_manager
    expose:
      - '9000'
    ports:
      - 9000:9000
    environment:
      ZK_HOSTS: 'zookeeper:2181'
      APPLICATION_SECRET: 'random-secret'
      command: -Dpidfile.path=/dev/null
    links:
      - kafka
      - zookeeper
    depends_on:
      - 'zookeeper'
      - 'kafka'
    networks:
      - net

  hue:
    container_name: hue
    image: gethue/hue
    ports:
      - '8088:8888'
    env_file:
      - ./hadoop.env
    volumes:
      - ./hue.ini:/usr/share/hue/desktop/conf/hue-overrides.ini
    depends_on:
      - namenode
    networks:
      - net

  # Jupyter Notebook
  # jupyter:
  #   image: jupyter/pyspark-notebook
  #   container_name: jupyter
  #   ports:
  #     - 8888:8888
  #   depends_on:
  #     - 'kafka'
  #   links:
  #     - kafka
  #   volumes:
  #     - ./spark-lab/:/home/jovyan/work/
  #   networks:
  #     - net

  jupyterlab:
    image: andreper/jupyterlab:2.1.4-spark-3.0.0
    container_name: jupyterlab
    ports:
      - 8888:8888
    volumes:
      - ./spark-lab/:/opt/workspace/
    networks:
      - net

volumes:
  hadoop_namenode:
  hadoop_datanode1:
  hadoop_datanode2:
  hadoop_datanode3:
  hadoop_historyserver:

networks:
  net:
